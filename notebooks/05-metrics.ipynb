{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078771e3",
   "metadata": {},
   "source": [
    "# **Metrics**\n",
    "\n",
    "This notebooks develops a brief theoretical and practical introduction to the metric Peak Signal-to-Noise Radio, Structural similarity index and Frechet inception Distance, GAN-specific, all of them arevery useful for comparing relationships between original and enhanced images and assessing super-resolution quality.\n",
    "\n",
    "Author:  \n",
    "@jwpr-dpr  \n",
    "22-04-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac725b",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "Super-resolution (SR) is the task of reconstructing a high-resolution (HR) image from a low-resolution (LR) input. Among the most powerful and popular models for this task are SRGANs — Super-Resolution Generative Adversarial Networks — which generate photorealistic high-resolution images by combining a content loss with an adversarial loss. While GAN-based models significantly improve the perceptual quality of images, they introduce a new challenge: how to evaluate the quality of the generated outputs effectively.\n",
    "\n",
    "Traditionally, super-resolution algorithms have been evaluated using pixel-wise similarity metrics like PSNR and SSIM, which compare the generated image to a known ground truth HR image. However, these metrics often fail to align with human visual perception. For example, images with high PSNR values can look overly smooth or blurry, while perceptually sharper images produced by SRGANs might score lower on PSNR despite looking more realistic.\n",
    "\n",
    "The goal of SR is not only to create an image that is numerically similar to the original HR image, but also to create one that looks natural and visually pleasing to a human observer. This dual objective leads to a conflict:\n",
    "\n",
    "* Pixel-wise metrics (e.g., PSNR, SSIM) reward faithful reproduction of pixel values, favoring blurrier outputs from traditional interpolation or CNN-based SR methods.\n",
    "\n",
    "* Perceptual metrics (e.g., FID, LPIPS) reward photorealism, encouraging GANs to \"hallucinate\" plausible textures and details, which may not exist in the original image but look realistic.\n",
    "\n",
    "Therefore, to fairly assess the performance of SRGANs, it's crucial to look beyond just PSNR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03275133",
   "metadata": {},
   "source": [
    "## **Overview of metrics**\n",
    "\n",
    "When evaluating the performance of super-resolution models—especially those using generative adversarial networks like SRGAN—it is essential to apply multiple metrics that capture different aspects of image quality. Each metric has its own assumptions, strengths, and weaknesses.\n",
    "\n",
    "In this section, we’ll introduce the three key metrics we’ll use to evaluate SRGAN performance: PSNR, SSIM, and FID.\n",
    "\n",
    "### PSNR (Peak Signal-to-Noise Ratio)\n",
    "PSNR quantifies the pixel-level difference between the SR image and the HR ground truth. It is based on the Mean Squared Error (MSE) and is expressed in decibels (dB). Higher PSNR generally indicates better fidelity, but not necessarily better visual quality.\n",
    "\n",
    "$$\n",
    "PSNR(I,\\hat{I}) = 10 * \\log_{10} \\left( \\frac{max^{2}_{i}}{mse(I,\\hat{I})} \\right)\n",
    "$$\n",
    "\n",
    "Where  \n",
    "$max_i$ is the maximum possible pixel value (e.g., 255 for 8-bit images)  \n",
    "$I$: the ground truth HR image  \n",
    "$\\hat{I}$: the predicted SR image  \n",
    "\n",
    "Strengths: Simple, fast, widely used\n",
    "\n",
    "Weaknesses: Penalizes perceptual deviations even if they look better to the human eye\n",
    "\n",
    "### SSIM (Structural Similarity Index)\n",
    "SSIM evaluates image similarity by comparing luminance, contrast, and structure. It considers perceptual aspects of vision, making it better than PSNR for many tasks involving textures and edges.\n",
    "\n",
    "Strengths: Correlates better with perceived visual quality\n",
    "\n",
    "Weaknesses: Still limited to local patch-wise comparisons\n",
    "\n",
    "### FID (Fréchet Inception Distance)\n",
    "FID is a distribution-based metric that compares the statistics of features extracted by a deep network (usually InceptionV3) between real HR images and generated SR images. It is widely used to evaluate GAN outputs and correlates well with human judgment.\n",
    "\n",
    "$$\n",
    "FID = ||\\mu_r - \\mu_g ||^2 + Tr(\\sum_r + \\sum_g - 2 \\cdot (\\sum_r \\sum_g)^1/2)\n",
    "$$\n",
    "\n",
    "With  \n",
    "$\\mu_r$,$ \\sum_r$ mean and covariance of real image features  \n",
    "$\\mu_g$, $\\sum_g$ mean and covariance of generated image features\n",
    "Strengths: Sensitive to visual realism and diversity\n",
    "\n",
    "Weaknesses: Requires a pre-trained model; sensitive to input preprocessing\n",
    "\n",
    "|Metric | Use Case | Best For | Fails At|\n",
    "|---|----|----|----|\n",
    "|PSNR | Basic image reconstruction | Measuring pixel-wise fidelity | Perceptual quality|\n",
    "|SSIM | Structure-aware tasks | Structural similarity | Complex textures|\n",
    "|FID | GAN-based models | Perceptual realism | Exact fidelity to original|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba50d89",
   "metadata": {},
   "source": [
    "## **How to create an evaluation function including all the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333d75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.models.inception import inception_v3\n",
    "from torchvision.transforms import Resize, Normalize, ToTensor, Compose\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from skimage.metrics import peak_signal_noise_ratio \n",
    "from skimage.metrics import structural_similarity \n",
    "from scipy.linalg import sqrtm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac19444",
   "metadata": {},
   "source": [
    "FID (Fréchet Inception Distance) is a statistical comparison of two feature distributions extracted from images — typically high-level activations from a pretrained InceptionV3 network. The quality of the FID score depends heavily on how you prepare the images before feeding them into InceptionV3.\n",
    "\n",
    "The network expects a certain input size and normalization, and deviating from that breaks the assumptions behind FID’s calculation. \n",
    "\n",
    "|Step | Why It Matters|\n",
    "|---|----|\n",
    "|Resize to 299x299 | InceptionV3 input requirement; ensures comparability with ImageNet features.|\n",
    "|Convert to tensor | Switches from HWC to CHW and rescales to [0, 1].|\n",
    "|Normalize | Brings range to [-1, 1], matching training-time normalization of the network.|\n",
    "\n",
    "If you skip or change these steps:\n",
    "\n",
    "* The Inception model will behave unpredictably.\n",
    "* Your feature distributions will be invalid, and so will your FID.\n",
    "* Your FID may look very low (good) or very high (bad), but won’t mean anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e89aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_fid(imgs, device):\n",
    "    # Normalize to [-1, 1] and resize to 299x299 (required by InceptionV3)\n",
    "    transform = Compose([\n",
    "        Resize((299, 299)),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5]*3, [0.5]*3)  # from [0,1] to [-1,1]\n",
    "    ])\n",
    "    imgs_t = torch.stack([transform(to_pil_image((img*255).astype(np.uint8))) for img in imgs])\n",
    "    return imgs_t.to(device)\n",
    "\n",
    "def calculate_fid(features1, features2):\n",
    "    mu1, sigma1 = np.mean(features1, axis=0), np.cov(features1, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(features2, axis=0), np.cov(features2, rowvar=False)\n",
    "    diff = mu1 - mu2\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return float(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c47cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_srgan(sr_images, hr_images, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates SRGAN results using PSNR, SSIM, and FID.\n",
    "\n",
    "    Args:\n",
    "        sr_images (List[np.ndarray]): Super-resolved images (H, W, C) in [0, 1]\n",
    "        hr_images (List[np.ndarray]): High-res ground truth images (H, W, C) in [0, 1]\n",
    "        device (str): \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"PSNR\": float, \"SSIM\": float, \"FID\": float}\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforms for InceptionV3: resize to 299x299 and normalize\n",
    "    inception_transform = Compose([\n",
    "        Resize((299, 299), interpolation=Image.BICUBIC),\n",
    "        ToTensor(),  # Converts [H, W, C] to [C, H, W] in [0, 1]\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # For [-1, 1] range\n",
    "    ])\n",
    "\n",
    "    # InceptionV3 model for FID (no aux logits, output features only)\n",
    "    inception = inception_v3(pretrained=True, transform_input=False)\n",
    "    inception.fc = torch.nn.Identity()  # Output 2048-D features\n",
    "    inception.eval().to(device)\n",
    "\n",
    "    psnr_scores, ssim_scores = [], []\n",
    "    sr_tensors, hr_tensors = [], []\n",
    "\n",
    "    for sr, hr in zip(sr_images, hr_images):\n",
    "        # PSNR & SSIM (on original resolution)\n",
    "        psnr = peak_signal_noise_ratio(hr, sr, data_range=1.0)\n",
    "        ssim = structural_similarity(hr, sr, win_size=3, multichannel=True, data_range=1.0)\n",
    "        psnr_scores.append(psnr)\n",
    "        ssim_scores.append(ssim)\n",
    "\n",
    "        # FID preprocessing\n",
    "        sr_img = Image.fromarray((sr * 255).astype(np.uint8))\n",
    "        hr_img = Image.fromarray((hr * 255).astype(np.uint8))\n",
    "        sr_tensor = inception_transform(sr_img)\n",
    "        hr_tensor = inception_transform(hr_img)\n",
    "        sr_tensors.append(sr_tensor)\n",
    "        hr_tensors.append(hr_tensor)\n",
    "\n",
    "    # Stack tensors and send to device\n",
    "    sr_batch = torch.stack(sr_tensors).to(device)\n",
    "    hr_batch = torch.stack(hr_tensors).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_sr = inception(sr_batch).cpu().numpy()\n",
    "        features_hr = inception(hr_batch).cpu().numpy()\n",
    "\n",
    "    # Compute FID\n",
    "    fid_score = calculate_fid(features_hr, features_sr)\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": np.mean(psnr_scores),\n",
    "        \"SSIM\": np.mean(ssim_scores),\n",
    "        \"FID\": fid_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310aa1d",
   "metadata": {},
   "source": [
    "This is a simple routine for loading images from CIFAR10 database. Loading images from a validation set would look very different from this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0fbd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def load_cifar_lr_hr_pairs(\n",
    "    dataset=\"CIFAR10\",\n",
    "    root=\"../data\",\n",
    "    split=\"test\",\n",
    "    upscale_factor=4,\n",
    "    max_images=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads HR images from CIFAR and generates corresponding SR (upscaled LR) images.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): \"CIFAR10\" (default) or \"CIFAR100\".\n",
    "        root (str): Directory where the dataset will be downloaded/stored.\n",
    "        split (str): \"train\" or \"test\".\n",
    "        upscale_factor (int): Factor to downscale and then upscale for LR simulation.\n",
    "        max_images (int): If set, limits number of images to load.\n",
    "\n",
    "    Returns:\n",
    "        sr_images (List[np.ndarray]): Upscaled LR images in [0, 1], shape (H, W, 3)\n",
    "        hr_images (List[np.ndarray]): Original CIFAR images in [0, 1], shape (H, W, 3)\n",
    "    \"\"\"\n",
    "    # Define dataset\n",
    "    DatasetClass = CIFAR10 \n",
    "    cifar_data = DatasetClass(\n",
    "        root=root,\n",
    "        train=(split == \"train\"),\n",
    "        download=True,\n",
    "        transform=T.ToTensor()  # Get tensor in [0, 1]\n",
    "    )\n",
    "\n",
    "    # Downscale and upscale transforms\n",
    "    downscale = T.Resize(32 // upscale_factor, interpolation=T.InterpolationMode.BICUBIC)\n",
    "    upscale = T.Resize(32, interpolation=T.InterpolationMode.BICUBIC)\n",
    "\n",
    "    sr_images, hr_images = [], []\n",
    "    for i, (img_tensor, _) in enumerate(cifar_data):\n",
    "        if max_images and i >= max_images:\n",
    "            break\n",
    "        img_pil = T.ToPILImage()(img_tensor)\n",
    "\n",
    "        # HR image (original)\n",
    "        hr = np.asarray(img_pil).astype(np.float32) / 255.0\n",
    "\n",
    "        # SR image (down-up simulation)\n",
    "        lr = downscale(img_pil)\n",
    "        sr = upscale(lr)\n",
    "        sr = np.asarray(sr).astype(np.float32) / 255.0\n",
    "\n",
    "        hr_images.append(hr)\n",
    "        sr_images.append(sr)\n",
    "\n",
    "    return sr_images, hr_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6494247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "PSNR: 21.1966\n",
      "SSIM: 0.6843\n",
      "FID: 184.9726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sr_images, hr_images = load_cifar_lr_hr_pairs(\n",
    "    dataset=\"CIFAR10\", split=\"test\", upscale_factor=4, max_images=1000)\n",
    "\n",
    "results = evaluate_srgan(sr_images, hr_images, device=\"cpu\")\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b5d18",
   "metadata": {},
   "source": [
    "## **Interpreting Results**\n",
    "\n",
    "1. PSNR: 21.20 dB\n",
    "PSNR (Peak Signal-to-Noise Ratio) measures the pixel-level similarity between your SR (super-resolved) images and HR (high-resolution ground truth) images.\n",
    "\n",
    "Higher is better\n",
    "\n",
    "Typical range:\n",
    "\n",
    "* 30 dB: Excellent (close to HR)\n",
    "* 25–30 dB: Good\n",
    "* 20–25 dB: Acceptable\n",
    "* <20 dB: Poor\n",
    "\n",
    "Interpretation:\n",
    "A PSNR of 21.20 dB is in the acceptable range. It means the SR images resemble the HR images somewhat, but with noticeable differences at the pixel level — possibly artifacts or blurring.\n",
    "\n",
    "2. SSIM: 0.6843\n",
    "SSIM (Structural Similarity Index) evaluates the perceptual similarity between SR and HR images based on structure, luminance, and contrast.\n",
    "\n",
    "Closer to 1 is better\n",
    "\n",
    "Typical range:\n",
    "\n",
    "* 0.90: Very high perceptual similarity\n",
    "* 0.75–0.90: Good\n",
    "* 0.60–0.75: Moderate\n",
    "* <0.60: Low\n",
    "\n",
    "Interpretation:\n",
    "An SSIM of 0.6843 suggests a moderate level of perceptual similarity. The general structure is preserved, but the finer details (textures, edges) may be degraded or different from the original.\n",
    "\n",
    "3. FID: 184.97\n",
    "FID (Fréchet Inception Distance) measures the distributional difference between the features of SR and HR images (using a pretrained model like InceptionV3). Lower is better.\n",
    "\n",
    "Lower is better\n",
    "\n",
    "Typical range (varies by dataset):\n",
    "\n",
    "* <10: Excellent (often achieved by the best models on large datasets)\n",
    "* 10–50: Good\n",
    "* 50–100: Fair\n",
    "* 100: Poor\n",
    "\n",
    "Interpretation:\n",
    "An FID of 184.97 is quite high, indicating your model-generated images are far from matching the distribution of real HR images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13cfb1",
   "metadata": {},
   "source": [
    "Of course, the mediocre-poor results we are seeing in FID are expected, as we are using images with very low resolution, this metric focusing on quality, is not going to perform well, in the other hand we are using downgraded images from a set of original images, reason why the metrics that focus in image integrity respect to the originals, perform much better. The important thing here is, we have managed to create a function that receives a batch of images and generates metrics succesfully. Now we can adequately evaluate our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd2ef8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
