{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3930ee",
   "metadata": {},
   "source": [
    "# **VGG-19**\n",
    "\n",
    "VGG19 is one of the most important CNNs. Its development has lead to tons of greater applications in the field of computer vision, specially in patern recognition. This notebook aims to give a brief resume of this model, and justify why it is used in our model\n",
    "\n",
    "Author:  \n",
    "@jwpr-dpr  \n",
    "15-04-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b609df",
   "metadata": {},
   "source": [
    "## **Introduction to VGG-19**\n",
    "\n",
    "Welcome to this tutorial on **VGG-19**, one of the most influential convolutional neural networks in computer vision! Developed by the **Visual Geometry Group (VGG)** at the University of Oxford, VGG-19 gained popularity thanks to its elegant and simple architecture built entirely from 3x3 convolution layers and 2x2 max-pooling layers.\n",
    "\n",
    "### What is VGG-19?\n",
    "\n",
    "VGG-19 is a deep convolutional neural network that consists of:\n",
    "\n",
    "- 19 weight layers: **16 convolutional layers** and **3 fully connected layers**\n",
    "- Small filter sizes: All convolutional layers use 3x3 filters with stride 1 and padding to preserve spatial resolution.\n",
    "- Simplicity in design: Repeated stacking of simple layers rather than using complex components.\n",
    "\n",
    "The model was introduced in the 2014 paper:  \n",
    "*Very Deep Convolutional Networks for Large-Scale Image Recognition* by Karen Simonyan and Andrew Zisserman.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use VGG-19?\n",
    "\n",
    "-  **Transfer learning**: Excellent for feature extraction and fine-tuning on smaller datasets.\n",
    "-  **Benchmarking**: A strong baseline model for many image classification tasks.\n",
    "-  **Style transfer**: VGG-19 is widely used in neural style transfer applications.\n",
    "\n",
    "Despite being more computationally expensive compared to newer architectures, VGG-19 remains an standard to complex and extensive visul applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36afee",
   "metadata": {},
   "source": [
    "## **Architecture Overview: VGG-19**\n",
    "\n",
    "The **VGG-19 architecture** is a deep convolutional neural network composed of 19 layers with learnable weights:\n",
    "- **16 convolutional layers**\n",
    "- **3 fully connected (dense) layers**\n",
    "\n",
    "It uses **very small filters (3√ó3)** and **max-pooling (2√ó2)** layers after blocks of convolutions. Each convolution layer uses **ReLU activation**, and the final classification is done via a **Softmax layer**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Layer Structure Summary\n",
    "\n",
    "The input image size is typically **224√ó224√ó3** (RGB).\n",
    "\n",
    "| Layer Block | Structure | Output Size |\n",
    "|-------------|-----------|-------------|\n",
    "| Input       | -         | 224√ó224√ó3   |\n",
    "| Conv Block 1 | 2√ó(Conv3-64) + MaxPool | 112√ó112√ó64 |\n",
    "| Conv Block 2 | 2√ó(Conv3-128) + MaxPool | 56√ó56√ó128 |\n",
    "| Conv Block 3 | 4√ó(Conv3-256) + MaxPool | 28√ó28√ó256 |\n",
    "| Conv Block 4 | 4√ó(Conv3-512) + MaxPool | 14√ó14√ó512 |\n",
    "| Conv Block 5 | 4√ó(Conv3-512) + MaxPool | 7√ó7√ó512 |\n",
    "| FC Layers    | Flatten ‚Üí FC-4096 ‚Üí FC-4096 ‚Üí FC-1000 | 1000 classes (ImageNet) |\n",
    "\n",
    "Note: Each convolution uses a 3√ó3 kernel, stride=1, and padding=1 (same padding).\n",
    "\n",
    "---\n",
    "\n",
    "###  Architectural Highlights\n",
    "\n",
    "-  **ReLU after every conv layer**\n",
    "-  **MaxPooling after each block**\n",
    "-  **No BatchNorm** in original version\n",
    "-  **No shortcuts or attention layers**‚Äîvery straightforward!\n",
    "-  No residuals, no depthwise separable convs‚Äîjust plain old convs and FCs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511e043",
   "metadata": {},
   "source": [
    "## **Implementing VGG-19 in Code (with PyTorch)**\n",
    "\n",
    "This step will be quite straight forward, as we eill be using the pretrained model that exists in repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f862bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\deep\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\deep\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full VGG-19 Architecture:\n",
      "\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "#from torchsummary import summary  \n",
    "\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "# Set to evaluation mode (we won't be training it)\n",
    "vgg19.eval()\n",
    "\n",
    "# Print the full architecture\n",
    "print(\"Full VGG-19 Architecture:\\n\")\n",
    "print(vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9921ed1",
   "metadata": {},
   "source": [
    "### üîç Notes:\n",
    "- We're loading **weights pretrained on ImageNet**.\n",
    "- The `vgg19` model has two parts:\n",
    "  - `features`: All convolutional and pooling layers\n",
    "  - `classifier`: The three fully connected layers used for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d425b5",
   "metadata": {},
   "source": [
    "## **Applications of VGG-19 in Deep Learning**\n",
    "\n",
    "Although VGG-19 is no longer state-of-the-art, it remains **widely used in practice** due to its simplicity and effectiveness. Below, we cover 4 core use cases:\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "We can use VGG-19 as a **feature extractor** by removing the final classification head and using the convolutional layers to obtain embeddings.\n",
    "\n",
    "This is useful for:\n",
    "- Custom classifiers\n",
    "- Similarity metrics\n",
    "- Clustering images by content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pretrained VGG-19\n",
    "vgg19 = models.vgg19(pretrained=True).features  # Only convolutional layers\n",
    "vgg19.eval()\n",
    "\n",
    "# Pass image through VGG-19 to get feature map\n",
    "with torch.no_grad():\n",
    "    features = vgg19(image_tensor)  # shape: [B, 512, 7, 7] for 224x224 input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379764cb",
   "metadata": {},
   "source": [
    "### Transfer Learning \n",
    "\n",
    "You can tune VGG-19 on your own dataset by\n",
    "* Freezing all features layers\n",
    "* Replacing and training a new classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Load full model\n",
    "model = models.vgg19(pretrained=True)\n",
    "\n",
    "# Freeze feature extractor\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier for your custom task\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 10),  # E.g. for CIFAR-10\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a98c8d",
   "metadata": {},
   "source": [
    "### Image Classification \n",
    "\n",
    "VGG-19 was originally trined on ImageNet and can be retrained on diverse image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744cda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Preprocess image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img = Image.open(\"your_image.jpg\")\n",
    "img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "output = model(img_tensor)\n",
    "prediction = output.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467b9dd",
   "metadata": {},
   "source": [
    "### Style Transfer (Iconic Use Case!)\n",
    "VGG-19 is the backbone of neural style transfer (NST) -This has become very famous lately, as people uses the possibility of generating images using different artists styles, so yeah, this is how they have been riping off Ghibli Studios-, where:\n",
    "\n",
    "* One image provides content\n",
    "* Another provides style\n",
    "* The model mixes both by optimizing pixel values\n",
    "\n",
    "Core idea:\n",
    "* Use content loss from deeper layers (e.g., conv4_2)\n",
    "* Use style loss from shallower layers (e.g., conv1_1, conv2_1, etc.)\n",
    "* This is often implemented using vgg19.features and computing Gram matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2575cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Hooking specific layers for content/style loss\n",
    "vgg = models.vgg19(pretrained=True).features.eval()\n",
    "\n",
    "# You can extract intermediate layers like this:\n",
    "def get_features(x, model, layers):\n",
    "    features = {}\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[name] = x\n",
    "    return features\n",
    "\n",
    "layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1'}\n",
    "features = get_features(input_img, vgg, layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f2bc5",
   "metadata": {},
   "source": [
    "## **Perceptual Loss with VGG-19**\n",
    "\n",
    "Perceptual Loss (also called **Feature Reconstruction Loss**) measures the difference between high-level feature representations of two images instead of raw pixel-wise differences.\n",
    "\n",
    "This is particularly useful in:\n",
    "- Super-resolution (SRGAN)\n",
    "- Style transfer\n",
    "- Inpainting / Image generation\n",
    "- Denoising\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Idea\n",
    "\n",
    "Use a **pretrained model like VGG-19**, and extract intermediate feature maps (e.g., from `conv3_3` or `conv4_2`), then compute L1 or L2 loss between those maps for two images:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{perceptual}}(x, \\hat{x}) = \\| \\phi_l(x) - \\phi_l(\\hat{x}) \\|_2^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\phi_l$  is the activation at layer $ l $\n",
    "- $ x $ is the ground truth image\n",
    "- $ \\hat{x} $ is the generated image\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Implementing Perceptual Loss with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d97205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer='conv4_2', resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        self.vgg_layers = models.vgg19(pretrained=True).features.eval()\n",
    "        \n",
    "        # Freeze the VGG weights\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Choose layer index corresponding to desired perceptual depth\n",
    "        self.layer_name_mapping = {\n",
    "            'conv1_1': 0,\n",
    "            'conv1_2': 2,\n",
    "            'conv2_1': 5,\n",
    "            'conv2_2': 7,\n",
    "            'conv3_1': 10,\n",
    "            'conv3_2': 12,\n",
    "            'conv3_3': 14,\n",
    "            'conv3_4': 16,\n",
    "            'conv4_1': 19,\n",
    "            'conv4_2': 21,\n",
    "            'conv4_3': 23,\n",
    "            'conv4_4': 25,\n",
    "            'conv5_1': 28\n",
    "        }\n",
    "\n",
    "        self.target_layer = self.layer_name_mapping[layer]\n",
    "        self.resize = resize\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Resize if needed\n",
    "        if self.resize:\n",
    "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            y = nn.functional.interpolate(y, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize to ImageNet stats\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(x.device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(x.device).view(1, 3, 1, 1)\n",
    "        x = (x - mean) / std\n",
    "        y = (y - mean) / std\n",
    "\n",
    "        # Forward until target layer\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            if i == self.target_layer:\n",
    "                break\n",
    "\n",
    "        # Compute perceptual loss\n",
    "        return self.criterion(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f894c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
