{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf30cd4",
   "metadata": {},
   "source": [
    "# **Grad-CAM**\n",
    "\n",
    "In this notebook is contained a brief explanation on grad-CAM technique for explainble AI and also an explanation of its aplication on SR-GANs\n",
    "\n",
    "Author:  \n",
    "@jwpr-dpr  \n",
    "17-04-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336af362",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "In recent years, deep learning has revolutionized the field of computer vision, enabling powerful models that can not only classify objects with human-level accuracy but also generate highly detailed images from low-resolution inputs. One such breakthrough is the Super-Resolution Generative Adversarial Network (SRGAN), which produces photorealistic high-resolution images by learning to hallucinate plausible textures.\n",
    "\n",
    "However, as these models grow more complex and powerful, they also become increasingly opaque. Understanding why a model makes a particular decision—or what parts of an image it focuses on—remains a major challenge. This lack of interpretability limits trust and hinders the adoption of deep learning systems in critical applications such as medical imaging or remote sensing.\n",
    "\n",
    "To address this, Grad-CAM (Gradient-weighted Class Activation Mapping) has emerged as a popular method for visualizing which parts of an image a convolutional neural network (CNN) considers important for its decisions. Originally designed for image classifiers, Grad-CAM highlights salient regions in an input image by backpropagating gradients from a target output through the network.\n",
    "\n",
    "In this notebook, we explore how Grad-CAM works and extend it beyond classification tasks to interpret generative models, specifically SRGANs. By applying Grad-CAM to components like the discriminator and the VGG-based perceptual loss, we can uncover valuable insights into what these networks consider \"realistic\" when generating high-resolution images.\n",
    "\n",
    "This tutorial is structured to provide both the theoretical foundation and hands-on implementation of Grad-CAM, guiding you step-by-step from basic visual explanations to advanced applications in SRGANs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67711a42",
   "metadata": {},
   "source": [
    "## **Basic Theory of Grad-CAM**\n",
    "Understanding how deep convolutional neural networks (CNNs) make decisions is crucial for debugging models, validating robustness, and building trust in AI systems. Grad-CAM (Gradient-weighted Class Activation Mapping) is a widely used technique for achieving this. It produces visual explanations by highlighting regions of the input image that are important for a particular prediction.\n",
    "\n",
    "### Intuition Behind Grad-CAM\n",
    "CNNs process an image through successive layers of convolutions to extract increasingly abstract features. By the time we reach the final convolutional layers, these feature maps encode high-level spatial information about objects in the image.\n",
    "\n",
    "Grad-CAM works by:\n",
    "\n",
    "1. Selecting the target class (e.g., \"cat\") or any scalar output from the model.\n",
    "2. Computing the gradient of this output with respect to the feature maps of a convolutional layer.\n",
    "3. Using these gradients to determine the importance of each feature map.\n",
    "4. Combining the weighted feature maps to create a heatmap of relevant regions in the image.\n",
    "\n",
    "This heatmap is then overlaid on the input image, providing a localized visual explanation.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Let $A^{k} \\in \\mathbb{R}^{\\mathbb{H} \\times \\mathbb{W}}$ be the $k$-th feature map of a selected convolutional layer and $y^c$ be the scalar output of interest\n",
    "\n",
    "Grad-CAM computes:\n",
    "1. Gradients of the target score with respect to feature maps:\n",
    "$$ \\frac{\\partial y^c}{\\partial A^{k}_{ij}} $$\n",
    "\n",
    "2. Global average pooling over spatial dimensions (i,j), to obtain importance weights\n",
    "$$ \\alpha_{k}^{c} = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A^{k}_{ij}} $$\n",
    "\n",
    "Where $Z = \\mathbb{H} \\times \\mathbb{W}$\n",
    "\n",
    "3. Weighted combination of feature maps followed by ReLU:\n",
    "\n",
    "$$L_{\\text{Grad-CAM}} = \\text{ReLU} \\left( \\sum_k \\alpha_{k}^{c} A^{k} \\right) $$\n",
    "* ReLU is applied to focus only on features that have a positive influence on the output class\n",
    "* The result $L_{\\text{Grad-CAM}}$ is a coarse heatmap that highlights important regions\n",
    "\n",
    "The last convolutional layer is typically used for Grad-CAM because it retains rich spatial information and has high-level semantic features. Earlier layers may provide more localized but less semantically meaningful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2891a1",
   "metadata": {},
   "source": [
    "## **Applying Grad-CAM to SRGANs**\n",
    "Grad-CAM is traditionally used with classification networks, where a scalar output (like a class logit) is naturally defined. But SRGANs are generative models, and the generator doesn't produce a scalar—it outputs an image.\n",
    "\n",
    "So how can we apply Grad-CAM in this context?\n",
    "\n",
    "To make Grad-CAM meaningful for SRGANs, we must rethink the target scalar output. In SRGANs, we can focus on two components that do produce scalar outputs:\n",
    "\n",
    "The discriminator, which predicts realism.\n",
    "\n",
    "The VGG-based perceptual loss, which measures similarity to the ground truth in feature space.\n",
    "\n",
    "Let’s walk through both options:\n",
    "\n",
    "### 1. Grad-CAM on the Discriminator\n",
    "The discriminator in SRGAN is a CNN classifier that distinguishes between real high-resolution (HR) images and generated super-resolved (SR) images. The discriminator outputs a scalar “realness” score → perfect for Grad-CAM. Applying Grad-CAM to this score tells us what parts of the image made it look real (or fake) to the discriminator.\n",
    "\n",
    "* How to apply it:  \n",
    "Pass the generated image to the discriminator. Choose the output before the sigmoid (the logit). Select a deep convolutional layer in the discriminator. Backpropagate the score through this layer. Use Grad-CAM to generate a heatmap over the image.\n",
    "\n",
    "* What we learn:  \n",
    "Where the discriminator \"looks\" to make its judgment. Whether it focuses on textures, edges, or artifacts. Which regions convince it the image is real (or fake).\n",
    "\n",
    "### 2. Grad-CAM via VGG-based Perceptual Loss\n",
    "SRGANs include a content loss term based on feature distances from a pretrained VGG network. This perceptual loss helps the generator produce images with texture and semantic fidelity. The perceptual loss is scalar (e.g., MSE between VGG feature maps). It represents how different the generated image is from the ground truth in perceptual space.\n",
    "\n",
    "* How to apply it:  \n",
    "Compute the perceptual loss between the SR and HR images using VGG. Choose an intermediate convolutional layer from VGG (e.g., conv3_3 or conv4_4). Backpropagate the perceptual loss through this layer. Apply Grad-CAM to see which areas contribute most to the loss.\n",
    "\n",
    "* What we learn:  \n",
    "What regions the generator fails to capture perceptually. Which textures or structures are causing high perceptual loss. Where improvements could be made.\n",
    "\n",
    "|Target | Pros | Cons\n",
    "|---|---|---|\n",
    "|Discriminator output | Directly shows realism regions | May focus on GAN-specific artifacts|\n",
    "|Perceptual loss | Highlights perceptual mismatch | Harder to interpret without ground truth|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5cce6",
   "metadata": {},
   "source": [
    "## **Grad-CAM Implementation for SRGAN**\n",
    "\n",
    "Let's build an implementation of Grad-CAM for SRGAN, focusing first on the discriminator, since it's the most straightforward application. By loading a generator, a discriminator and a low resolution image to super-resolve. We’ll set up hooks to Store the feature maps from a target convolutional layer and store the gradients of the output with respect to these maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb2a31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the requiered libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9b47770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the simple generator and discriminator gotten from the 03-Adversarial-loss.ipynb notebook\n",
    "\n",
    "# === Device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to match training\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root=\"../data\", train=False, download=True, transform=transform)\n",
    "random_index = random.randint(0, len(mnist_data) - 1)\n",
    "lr_img, label = mnist_data[random_index]\n",
    "lr_img = lr_img.view(1, -1).to(device)  # Flatten to (1, 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Define Generator and Discriminator (same as used in training) ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bb7bd",
   "metadata": {},
   "source": [
    "> Now, before continue, please notice that we are keeping the same simple Generator, but the discriminator is diferent. Why is that?\n",
    "\n",
    "The reason is because we can't use Grad-CAM with only fully connected layers.\n",
    "\n",
    "Grad-CAM needs spatial information to work. FC layers flatten everything, losing this structure.\n",
    "\n",
    "* Fully Connected layers → vector output\n",
    "* Convolutional layers → feature maps with spatial dimensions\n",
    "\n",
    "Grad-CAM requires:\n",
    "\n",
    "* Feature maps from a convolutional layer (shape: [C, H, W])\n",
    "* Gradients w.r.t. those maps to know which spatial regions matter\n",
    "\n",
    "So, if your discriminator is just fully connected layers, you can’t compute a spatial heatmap, because there’s no spatial info left to highlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "class ConvDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvDiscriminator, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),  # 28x28 → 14x14\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 14x14 → 7x7\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.features(x)\n",
    "        out = self.classifier(feats)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7772e0",
   "metadata": {},
   "source": [
    "Why a Convolutional Discriminator Is Needed for Grad-CAM?\n",
    "If you're using Grad-CAM to interpret a discriminator (as in a GAN or SRGAN setup), you must design the discriminator with convolutional layers, so:\n",
    "\n",
    "* You have feature maps with spatial resolution\n",
    "* You can compute gradients w.r.t. those maps\n",
    "* You can generate meaningful visual explanations for where the discriminator is focusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88dbd5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDiscriminator(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6272, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 3: Load Pretrained Models ===\n",
    "latent_dim = 100\n",
    "img_dim = 28 * 28\n",
    "\n",
    "g = Generator(latent_dim, img_dim).to(device)\n",
    "d = ConvDiscriminator().to(device)\n",
    "\n",
    "g.load_state_dict(torch.load(\"../test/gen_epoch_14.pth\", map_location=device))\n",
    "g.eval()\n",
    "d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "904d83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(1, latent_dim).to(device)\n",
    "with torch.no_grad():\n",
    "    sr_img = g(z).detach().squeeze()  # Output: (1, 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc736142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Grad-CAM Implementation ===\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Make sure input is in the form [B, C, H, W]\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            if x.ndim == 1:  # [784]\n",
    "                x = x.view(1, 1, 28, 28)\n",
    "            elif x.ndim == 2:  # [1, 784]\n",
    "                x = x.view(-1, 1, 28, 28)\n",
    "            elif x.ndim == 3:  # [1, 28, 28]\n",
    "                x = x.unsqueeze(0)\n",
    "            elif x.ndim == 4:\n",
    "                pass  # already [B, C, H, W]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported input shape: {x.shape}\")\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "\n",
    "        x = x.requires_grad_()\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "\n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        output.backward(torch.ones_like(output))\n",
    "\n",
    "        # Global average pooling of gradients\n",
    "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Resize and normalize\n",
    "        cam = F.interpolate(cam, size=(28, 28), mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3714762",
   "metadata": {},
   "source": [
    "With this implementation, we can now visualize the model behaviour and understand what is happening for it to make the decisions it is making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc55328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_img shape before: torch.Size([1, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 6: Run Grad-CAM ===\n",
    "target_layer = 'features.3'  # LeakyReLU after second Conv2d\n",
    "grad_cam = GradCAM(d, d.features[3])  # use second conv layer\n",
    "cam_output = grad_cam(sr_img)\n",
    "\n",
    "# === Plotting ===\n",
    "def show_img(img):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    # Convert tensor to numpy\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.detach().cpu().numpy()\n",
    "\n",
    "    # Remove batch dimension if present\n",
    "    if img.ndim == 2 and img.shape[0] == 1:\n",
    "        img = img[0]\n",
    "\n",
    "    # Flattened? Reshape it\n",
    "    if img.ndim == 1 and img.shape[0] == 784:\n",
    "        img = img.reshape(28, 28)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "print(f\"lr_img shape before: {lr_img.shape}\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Low-Res MNIST\")\n",
    "plt.imshow(show_img(lr_img), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"SR Output\")\n",
    "plt.imshow(show_img(sr_img), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Grad-CAM (Discriminator)\")\n",
    "plt.imshow(show_img(sr_img), cmap='gray')\n",
    "plt.imshow(cam_output, cmap='jet', alpha=0.5)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd7bf6",
   "metadata": {},
   "source": [
    "Once again, I encourage you not to place excessive emphasis on the results, as we are utilizing models with limited training for academic purposes. Instead, the focus should be on theoretical concepts and programming structure. I hope this material serves as a valuable guide for future projects in explainable AI. The ability of researchers to instill trust in models will be a key factor in achieving a proper and ethical integration of diverse methodologies across various industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bce2d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
